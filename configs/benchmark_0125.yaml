# Benchmark configuration for schema comparison experiments
#
# This config demonstrates the 5 schema comparison modes from ROADMAP.md:
# 1. In one word (no schema) - generic categorization without specific categories
# 2. In one word (oracle schema) - using schema from annotations
# 3. In one word (proposed schema) - dynamically generated schema
# 4. Pseudologit (oracle schema) - using schema from annotations
# 5. Pseudologit (proposed schema) - dynamically generated schema
#
# Usage:
#   uv run python -m multiview.benchmark.run configs/benchmark_schema_comparison.yaml
#
# Quick test (fewer docs and triplets):
#   Edit max_docs: 50, max_triplets: 25 below

run_name: "benchmark_0125"

logging:
  level: DEBUG
  output_file: "outputs/logs/${run_name}.log"

# Auto-visualization configuration
auto_visualize: true  # Generate visualizations after evaluation (default: true)
visualization:
  reducers: ["tsne", "umap", "pca", "som", "dendrogram"]  # All available reducers (graph & heatmap always available)
  thumbnails: true    # Use thumbnails for image datasets and GSM8K (default: true)
  output_dir: "viz"   # Output directory for visualizations (default: viz)

reuse_cached_triplets: true
seed: 42
use_cache: true
step_through: false

# Task configuration
tasks:
  defaults:
    max_docs: 200
    max_triplets: 50
    num_synthetic_docs: 25
    triplet_style: "lm_all"
    candidate_strategy: "multi"
    use_spurious_hard_negs: true
    embedding_preset: "hf_qwen3_embedding_8b"
    max_num_candidates: 10
    n_schema_samples: 10
    rate_triplet_quality: true
    min_triplet_quality: 2
    # Consistency validation (NEW) - ensures swapped (a,c,b) gets Invalid rating
    validate_consistency: true        # Enable consistency check (default: true)
    consistency_max_invalid: 1        # Swapped triplet must be rated ≤1 Invalid (default: 1, strict)
    prelabeled_selection: "hard_negatives"

  task_list:
    - document_set: gsm8k
      criterion: final_expression
      triplet_style: lm_tags

    - document_set: gsm8k
      criterion: problem_type
      triplet_style: lm_tags
      num_synthetic_docs: 0

# Evaluation methods - all 5 schema comparison modes
methods_to_evaluate:

  # # ============================================================================
  # # IN-ONE-WORD METHODS
  # # ============================================================================

  # in_one_word:
  #   # 1. In one word (no schema) - Generic categorization without specific categories
  #   - name: inoneword_no_schema
  #     preset: inoneword_hf_qwen3_8b
  #     category_context: "Question: Categorize this text in one word."

  #   # 2. In one word (oracle schema) - Uses schema from annotations (lm_all)
  #   # This will automatically extract category_schema from task.document_annotations
  #   - name: inoneword_oracle_schema
  #     preset: inoneword_hf_qwen3_8b
  #     # No category_context specified - will use annotations automatically

  #   # 3. In one word (proposed schema) - Run multiple trials with fresh schemas
  #   # This generates a new schema during evaluation without using the oracle
  #   # Each trial gets a separate row: inoneword_proposed_trial1, inoneword_proposed_trial2, etc.
  #   - name: inoneword_proposed
  #     preset: inoneword_hf_qwen3_8b
  #     generate_schema: true
  #     n_schema_samples: 10   # Number of docs to sample for schema generation
  #     num_trials: 10          # Run 10 times with different schemas (for averaging)

  # ============================================================================
  # PSEUDOLOGIT METHODS
  # ============================================================================

  pseudologit:
    # # 4. Pseudologit (oracle schema) - Uses schema from annotations
    # # Extracts classes from the oracle category_schema in annotations
    # - name: pseudologit_oracle_schema
    #   preset: pseudologit_gemini_n3
    #   use_oracle_schema: true

    # 5. Pseudologit (proposed schema) - Run multiple trials with fresh schemas
    # This generates a new schema during evaluation without using the oracle
    # Each trial gets a separate row: pseudologit_proposed_trial1, pseudologit_proposed_trial2, etc.
    - name: pseudologit_proposed
      preset: pseudologit_gemini_n3
      generate_schema: true
      n_schema_samples: 10   # Number of docs to sample for schema generation
      num_trials: 3          # Run 10 times with different schemas (for averaging)

    # # Custom schema - dict format (different classes file per task)
    # - name: pseudologit_custom_schema
    #   preset: pseudologit_gemini_n3
    #   classes_file:
    #     gsm8k_final_expression: prompts/custom/gsm8k_classes.json
    #     # onion_headlines_joke_type: prompts/custom/onion_joke_classes.json
    #     # Can also use just criterion name as key:
    #     # arithmetic_operations: prompts/custom/gsm8k_arithmetic_classes.json
    #     # joke_type: prompts/custom/onion_joke_classes.json

  # ============================================================================
  # CUSTOM SCHEMA METHODS (optional - for testing specific schemas)
  # ============================================================================

  # Uncomment to test custom schemas:
  # Supports both string (same for all tasks) and dict (per-task) formats

  # in_one_word:
  #   # Custom schema - dict format (different schema per task)
  #   - name: inoneword_custom_schema
  #     preset: inoneword_hf_qwen3_8b
  #     category_context:
  #       gsm8k_arithmetic_operations: "Categories: addition, subtraction, multiplication, division, mixed\nQuestion: Categorize this text in one word."
  #       onion_headlines_joke_type: "Categories: satire, irony, absurdist, parody, wordplay\nQuestion: Categorize this text in one word."
  #       # Can also use just criterion name as key:
  #       # arithmetic_operations: "Categories: ..."
  #       # joke_type: "Categories: ..."

  # # ============================================================================
  # # BASELINE METHODS (for comparison)
  # # ============================================================================

  embeddings:
    - name: qwen3_8b_with_instructions
      preset: instr_hf_qwen3_embedding_8b
    - name: qwen3_8b_no_instructions
      preset: hf_qwen3_embedding_8b


  bm25:
    # BM25 lexical baseline
    - name: bm25_baseline
      preset: bm25_lexical

  # query_relevance_vectors:
  #   - name: qrv_gemini_openai_k10_dev25
  #     expansion_preset: query_relevance_scores_gemini
  #     embedding_preset: openai_embedding_small
  #     num_expansions: 10
  #     dev_set_size: 5
  # # Document rewriting: Generate summaries and compare them
  # document_rewrite:
  #   # Embeddings over summaries
  #   - name: dr_gemini_lite_openai_small
  #     embedding_preset: openai_embedding_small
  #     summary_preset: document_summary_gemini_flash
# Output directory
outputs_dir: outputs/${run_name}

# Notes:
# ------
# Schema modes supported:
# - "no schema" = generic prompt without specific categories
# - "oracle schema" = the schema generated during annotation (lm_all or lm_tags)
# - "proposed schema" = freshly generated schema (oracle withheld)
# - "custom schema" = manually specified categories/classes file
#     - String format: same schema for all tasks
#     - Dict format: different schema per task (key: {dataset}_{criterion} or just {criterion})
# - num_trials: N creates N separate rows (method_trial1, method_trial2, ...)
#
# Oracle schema works with:
#   - lm_all: creates category_schema with discrete categories
#   - lm_tags: creates tag_schema with binary tags
#
# Expected results table (see outputs/schema_comparison/summary_table.json):
#   Method                             | Accuracy
#   -----------------------------------|----------
#   inoneword_no_schema                | X.XX
#   inoneword_oracle_schema            | X.XX
#   inoneword_proposed_trial1          | X.XX
#   inoneword_proposed_trial2          | X.XX
#   ...                                | ...
#   inoneword_proposed_trial10         | X.XX
#   pseudologit_oracle_schema          | X.XX
#   pseudologit_proposed_trial1        | X.XX
#   pseudologit_proposed_trial2        | X.XX
#   ...                                | ...
#   pseudologit_proposed_trial10       | X.XX
#   qwen3_8b_baseline                  | X.XX
#   bm25_baseline                      | X.XX
#
# To add custom schemas, uncomment the custom schema section above.
# Use compute_trial_statistics() or pandas to get mean ± std across trials
