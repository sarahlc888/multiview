# Benchmark configuration for GSM8K arithmetic evaluation
# Goal: Generate 256 triplets and evaluate with 4 methods
run_name: "benchmark_debug_analogies"
logging:
  level: DEBUG
  output_file: "temp.${run_name}.log"  # Log to console only
seed: 42
use_cache: true
step_through: false
# Task configuration
tasks:
  defaults:
    max_docs: 20  # Need more docs than triplets for good sampling
    max_triplets: 4
    num_synthetic_docs: 8    # Absolute number of synthetic docs to generate
    # max_docs: 100  # Need more docs than triplets for good sampling
    # max_triplets: 20
    # num_synthetic_docs: 10    # Absolute number of synthetic docs to generate
    # For quick testing, use: max_docs: 10, max_triplets: 2
    triplet_style: "lm_all"  # Rich annotations (categories + tags + summaries)
    add_synthetic_docs: false

    # Triplet creation parameters for lm_all style
    candidate_strategy: "multi"  # BM25 + embedding + Jaccard
    use_spurious_hard_negs: true   # Include spurious hard negatives in candidate pool
    embedding_preset: "hf_qwen3_embedding_8b"
    max_num_candidates: 10  # Max candidates shown to LM judge (per strategy for 'multi')
    n_schema_samples: 10

    # Quality filtering parameters
    rate_triplet_quality: true  # Enable LM-based quality rating
                                # When annotations exist, automatically compares
                                # ratings with/without annotations for insight
    min_triplet_quality: null # 3  # Filter to keep only triplets with quality >= 3 (trivial or ideal)
                            # 1=invalid, 2=ambiguous, 3=trivial, 4=ideal
                            # Set to null to rate but not filter

  task_list:
    - document_set: analogies
      dataset_config: bats
      criterion: analogy_type
      triplet_style: prelabeled  # Use prelabeled triplets (based on pre-existing criterion values)
      prelabeled_selection: hard_negatives  # "random" or "hard_negatives" (BM25-based)
      max_triplets: 10

# Evaluation methods configuration
methods_to_evaluate:
  # Method 1: BM25 baseline (lexical similarity)
  bm25:
    - name: bm25_lexical

  # Method 2: LM Judge Triplet (Gemini direct comparison)
  lm_judge_triplet:
    - name: gemini_flash_lite__triplet_with_annotation
      preset: lmjudge_triplet_plaintext_binaryhard_with_annotation_gemini
    - name: gemini_flash_lite_triplet_no_annotation
      preset: lmjudge_triplet_plaintext_binaryhard_gemini
    - name: gemini_flash_triplet_with_annotation
      preset: lmjudge_triplet_plaintext_binaryhard_with_annotation_gemini_flash
    - name: gemini_flash_triplet_no_annotation
      preset: lmjudge_triplet_plaintext_binaryhard_gemini_flash

  # Method 3 & 4: Embeddings with/without instructions
  embeddings:
    # WITH instruction prefix (default)
    - name: qwen3_8b_with_instructions
      preset: hf_qwen3_embedding_8b
      # Uses default instruction: "Represent this query for retrieval: "

    # WITHOUT instruction prefix (override to null)
    - name: qwen3_8b_no_instructions
      preset: hf_qwen3_embedding_8b
      preset_overrides:
        embed_query_instr_template: null  # Remove instruction prefix
