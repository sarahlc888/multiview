# will be used in output results/logs dir name
run_name: "default_benchmark"
logging:
  level: DEBUG
  output_file: outputs/logs/benchmark.log
seed: 42
use_cache: true

# which tasks to run
# (the task configs contain the information about how triplets are created)
tasks:
  # Global defaults for all tasks
  # Per-task settings override these defaults
  defaults:
    max_docs: 10 # 1000              # Maximum number of documents to load per task
    max_triplets: 2 # 500           # Maximum number of triplets to create per task
    triplet_style: "random" # "lm"         # How to sample triplets (random, lm)
    add_synthetic_docs: false   # Whether to add synthetic documents to the task

  # Task list
  task_list:
    - document_set: gsm8k
      criterion: arithmetic
      # Optional per-task overrides:
      # max_docs: 500
      # max_triplets: 100
      # triplet_style: random
    - document_set: gsm8k
      criterion: final_answer_units
    - document_set: crossword_clues
      criterion: clue_type
    - document_set: rocstories
      criterion: num_character_entities

# which models/methods to evaluate
# (listed by type)
# (we can evaluate any method that creates a similarity score between two texts)
# TODO: rename
methods_to_evaluate:
  bm25:
    - bm25
    - bm25_annotated
  embedding_models:
    - provider: 
      model: 
  instruct_embedding_models:
    - 
  lm_judge_triplet:
    - 
  lm_judge_pair:
    - 
