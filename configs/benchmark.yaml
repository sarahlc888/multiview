# Benchmark configuration for GSM8K arithmetic evaluation
# Goal: Generate 256 triplets and evaluate with 4 methods
run_name: "benchmark_debug"
logging:
  level: DEBUG
  output_file: "outputs/logs/${run_name}.log"
seed: 42
use_cache: true
step_through: false # true  # Enable breakpoints for debugging (set to true to step through code)

# Task configuration
tasks:
  defaults:
    max_docs: 500  # Need more docs than triplets for good sampling
    max_triplets: 256
    num_synthetic_docs: 100    # Absolute number of synthetic docs to generate


    # max_docs: 100  # Need more docs than triplets for good sampling
    # max_triplets: 20
    # num_synthetic_docs: 10    # Absolute number of synthetic docs to generate

    # For quick testing, use: max_docs: 10, max_triplets: 2
    triplet_style: "lm_all"  # Rich annotations (categories + tags + summaries)
    add_synthetic_docs: false

    # Triplet creation parameters for lm_all style
    candidate_strategy: "multi"  # BM25 + embedding + Jaccard
    use_spurious_hard_negs: true   # Include spurious hard negatives in candidate pool
    embedding_preset: "hf_qwen3_embedding_8b"
    max_num_candidates: 5  # Max candidates shown to LM judge (per strategy for 'multi')
    n_schema_samples: 4

    # Quality filtering parameters
    rate_triplet_quality: true  # Enable LM-based quality rating
                                # When annotations exist, automatically compares
                                # ratings with/without annotations for insight
    min_triplet_quality: null # 3  # Filter to keep only triplets with quality >= 3 (trivial or ideal)
                            # 1=invalid, 2=ambiguous, 3=trivial, 4=ideal
                            # Set to null to rate but not filter

  task_list:
    - document_set: gsm8k
      criterion: arithmetic
      add_synthetic_docs: true  # Enable synthetic document generation for GSM8K
      # Optional per-task overrides:
      # max_docs: 500
      # max_triplets: 100
      # triplet_style: random
    # - document_set: gsm8k
    #   criterion: final_answer_units
    # - document_set: crossword_clues
    #   criterion: clue_type
    # - document_set: rocstories
    #   criterion: num_character_entities

# Evaluation methods configuration
methods_to_evaluate:
  # Method 1: BM25 baseline (lexical similarity)
  bm25:
    - name: bm25_lexical

  # Method 2: LM Judge Triplet (Gemini direct comparison)
  lm_judge_triplet:
    - name: gemini_flash_triplet_with_annotation
      preset: lmjudge_triplet_plaintext_binaryhard_with_annotation_gemini
    - name: gemini_flash_triplet_no_annotation
      preset: lmjudge_triplet_plaintext_binaryhard_gemini

  # Method 3 & 4: Embeddings with/without instructions
  embeddings:
    # WITH instruction prefix (default)
    - name: qwen3_8b_with_instructions
      preset: hf_qwen3_embedding_8b
      # Uses default instruction: "Represent this query for retrieval: "

    # WITHOUT instruction prefix (override to null)
    - name: qwen3_8b_no_instructions
      preset: hf_qwen3_embedding_8b
      preset_overrides:
        embed_query_instr_template: null  # Remove instruction prefix
