# Test configuration for quality rating comparison (with vs without annotations)
# This config demonstrates the compare_quality_with_without_annotations feature,
# which rates triplets twice to assess whether annotation summaries help the LM
# make better quality judgments.
run_name: "test_quality_comparison"
logging:
  level: INFO
  output_file: "outputs/logs/${run_name}.log"
seed: 42
use_cache: true
step_through: false

# Task configuration
tasks:
  defaults:
    max_docs: 20         # Small dataset for testing
    max_triplets: 5      # Just a few triplets
    triplet_style: "lm_all"
    add_synthetic_docs: false

    # Triplet creation parameters
    candidate_strategy: "multi"
    use_spurious_hard_negs: true
    embedding_preset: "hf_qwen3_embedding_8b"
    max_num_candidates: 5
    n_schema_samples: 4

    # Quality rating
    # When annotations exist, comparison mode is AUTOMATIC - rates triplets
    # twice (with/without annotations) to assess annotation utility
    rate_triplet_quality: true
    min_triplet_quality: 3  # Filter to keep only quality >= 3

  task_list:
    - document_set: gsm8k
      criterion: arithmetic

# Evaluation methods - just a few for testing
methods_to_evaluate:
  bm25:
    - name: bm25_lexical

  lm_judge_triplet:
    - name: gemini_flash_triplet_with_annotation
      preset: lmjudge_triplet_plaintext_binaryhard_with_annotation_gemini
