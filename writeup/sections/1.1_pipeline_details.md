
Creating data for conditional similarity evaluation poses different challenges for different documents and criteria.

We include a few toy tasks to present a proof of concept, as well as more complex tasks to demonstrate the scalability/generalizability of the data generation approach.

We focus on a few levels of criteria complexity
0. **toy invariance**:
	- Trivial to solve with a query rewriter, but may be useful learning representations that are invariant to spurious factors
    - It's trivial to create toy triplets that focus on semantic invariance

	- Recipes
		- Data augmentation style strategies that test semantic invariance to spurious factors
		- Sentences by "third word of the sentence"
		- Two sentences, by the second sentence
		- it's possible to make this data in a ton of ways if it's just about invariance
			- criteria
			- the second sentence
			- and then add random noise for the first sentence
			- but that's so contrived and also query expansion solves it?
	- It's possible to create a large number of toy tasks, but the goal is to move beyond simple multi-hop or extractive tasks and instead make progress toward finding "surprising" semantic matches.
1. **extractive categorical labels**:
    -
    - Example: We could literally extract the word count for every example, and then ask the language model to create a triple based off of the top 50 candidates or so for any given an anchor.
	- Images by specific attribute
		- **Extractible attributes are not very interesting**... Can we do this for things that are harder ... We want to learn invariance but not the super toy kind
		- This also basically tests the strength of the encoder
		- Extractable attributes
			- Is there a figure in the scene?
			- How many characters are in the story ?
			- What is the first letter of the main character's name?
    - Simple reasoning over categorical criteria
        - multi-hop data
            - e.g. analogies
        - For example, simple tasks like "which emotion is most prevalent within this movie review" can be extracted in a near-deterministic fashion using a strong LM judge.
2. **tagg-able**:
    - However, more complex criteria may be more difficult to assess and represent.
2. **sparse matches**
    - Simple reasoning with sparse matches
	- Sparse matches
    - Some criteria may also have very sparse matches, e.g. finding "surprising" semantic matches.

2. **interpretive**:
    - Complex reasoning
	- Schema is not clear

    - For complex criteria, it is difficult to summarize how the document relates to the criteria
