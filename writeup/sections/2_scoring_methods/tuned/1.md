
## Method: tuned query rewriter with frozen embedding model
- same as above, but tuned
- a solid local embedder model + gpt 5 mini query rewriter
- + tune the rewriter in various ways (e.g. GEPA)
- training a query rewriting "advisor model"
	- ==advisor models framing is cool? but query rewriting is nnot realistic in terms of cost? could be a good teacher though==
	- "Query rewriting is useful for more than just search" ... but the cost is intractible?
		- I'm honestly so confused about the query rewriting thing and why it's useful
		- It's not practical to do it for a huge corpus
		- Well unless it's like a 4B model in which case it is super practical to do so
		- Maybe we should distill query rewriting into the embedding model ?
		- Frame the query rewriting thing as a way to generate data. And focus on what cool examples this might help us unearth?


- "advisor rewriters" (advisor models for query rewriting / expansion)
