


## Method: multiview embeddings

- method
	- For a newly provided criteria...
		- (1) Generate some new queries
		- (2) Select the most relevant queries from our query bank + new queries, using LLM judge scores
		- (3) Embed everything, and then produce our score embeddings as usual
- conceptual slop
	- Points of reference
	- Things relative to other things
	- Signposts
	- Landmark embeddings

==landmark = prototype = template== from https://cs231n.github.io/linear-classify/ "interpretation of ..."



- landmark score vectors / similarity vectors
	- Instruction tuned embedding models -> limited sensitivity
	- Retrievers are better
	- What if we hijack the query expansion step? We can check where the embedding sits relative to a set of "landmarks/prototypes/queries"
